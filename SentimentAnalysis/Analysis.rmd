---
title: "Sentiment Analysis.Rmd"
author: "Meena Muthusubramanian"
date: "10/29/2020"
output:
  pdf_document: default
  html_document: default
  code_folding: hide
---





```{r global options, include = FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE, cache = TRUE)
```



# Sentiment analysis and tf_idf

## Obtaining data

Navigating to this Github repository and selecting a new text dataset to analyze: https://github.com/niderhoff/nlp-datasets


## Processing data
1. Importing your data into R
2. Performing  additional cleaning tasks as needed.


```{R}
#Installing packages needed for text analysis.
loadPkg = function(toLoad){
  for(lib in toLoad){
    if(! lib %in% installed.packages()[,1])
    { install.packages(lib, repos='http://cran.rstudio.com/') }
    suppressMessages( library(lib, character.only=TRUE) ) }
}

packs=c('tidyverse', 'tidytext', 'textdata')

loadPkg(packs)

#Reading the input data csv file 
library(readr)
disaster_res_msg_val <- read_csv("disaster_res_msg_val.csv")
View(disaster_res_msg_val)

```

## Getting to know the data - dimensions, column names and more information about the data

```{r}
dim(disaster_res_msg_val)

names(disaster_res_msg_val)

head(disaster_res_msg_val)
```




## Word count
1. Tokenizing and generating a word count.



```{r}
#Text analysis with unnest_tokens for splitting text into simple objects and returns a df
disaster_words <- disaster_res_msg_val %>% select(id, message) %>% unnest_tokens(word, message)
head(disaster_words)
```


```{r}
disaster_words %>% count(word, sort = T) %>% slice(1:15) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")
```

2. Using the `TidyText` package, removing stop words and generate a new word count.

```{r}
head(stop_words)
```

```{r}
#Getting rid of words from the dataframe that match the TidyText stop words
better_disaster_words <- disaster_words %>% anti_join(stop_words)
```


```{r}
better_disaster_words %>% count(word, sort = T) %>% slice(1:15) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")

```


From the above graph, we can exclude words like "http", "affected" which I feel are  just stop words. 


3. Creating a visualization of the word count distribution and interpreting my results.

```{r}
personal_stop_words <- stop_words %>% select(-lexicon) %>% 
  bind_rows(data.frame(word = c("http", "affected","sandy")))

better_disaster_words <- disaster_words %>% anti_join(personal_stop_words)

better_disaster_words %>% count(word, sort = T) %>% slice(1:15) %>% 
  ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Words")

```


I have removed the stop words although still I find words like "haiti" which might have any meaning according to the content. But since it sounds like a  new word, I don't wish to exclude it because it might have significant meaning. 

## Tf-idf
1. Generating a tf-idf measure of words in your dataset.

Here, the first input is the word and the second is the id and the word.


```{r}
idf_words <- disaster_res_msg_val %>% select(id, message) %>% 
  unnest_tokens(word,message) %>% count(id, word, sort = T)

better_idf_words <- idf_words %>% anti_join(personal_stop_words)

disaster_length <- better_idf_words %>% group_by(id) %>% summarize(total = sum(n()))

better_idf_words <- left_join(better_idf_words, disaster_length)
```


```{r}
tfidf_words <- better_idf_words %>% bind_tf_idf(word, id, n)

tfidf_words %>% arrange(desc(tf_idf)) %>% head()
```

The more frequently the words are used,  the shorter the words is. 



## Sentiment analysis


Loading the pre-trained model containing word=emotions associations.
```{r}
sentiments <- get_sentiments("nrc")

df_sentiments <- better_disaster_words %>% left_join(sentiments)

df_sentiments_filtered <- df_sentiments %>% filter(!is.na(sentiment)) %>% group_by(sentiment) %>% summarize(n = n())

df_sentiments_filtered %>% 
  ggplot(aes(x = reorder(sentiment, n, function(n) -n), y = n)) + 
  geom_bar(stat = "identity") + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  xlab("Sentiments")

```

## Interpretation
Write a paragraph interpreting the word count, tf-idf, and sentiment of the words in your dataset. What do these tell us about the conversations surrounding your hashtag (if anything)? How could you improve this analysis?

I see that in the tf_idf analysis, there are some words displayed which don't have a meaning. It refelects the inverse document frequence which is  a numerical measure to reflect how important a word is in a document I believe it is calculated by figuring out how many times a word appears in a document and the inverse documnet frequency  if the word across a set of documents. Sentiment analysis is often used in business to detect sentiment in social data, gauge brand reputation, and understand customers.It's the process of analysing online pieces of writing to determine the emotional tone they carry. In simple words, sentiment analysis is used to find the author's attitude towards something. Sentiment analysis tools categorize pieces of writing as positive, neutral, or negative. In my specific data set, I find that it's positive while there are few negative emotions, positive leads the graph.

